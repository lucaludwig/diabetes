# -*- coding: utf-8 -*-
"""NLP Assignment 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q-D22Z93i4nMWiJifQYTgcoMEQhB4uTt
"""

# import needed libraries
import pandas as pd
import re
import random
from prettytable import PrettyTable
import textwrap 
import nltk

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline

from sklearn.metrics import accuracy_score
from sklearn import metrics

# loading the datasets
emotions = pd.read_excel('Diabetes-classification.xlsx', sheet_name='Emotions')
patient_journey = pd.read_excel('Diabetes-classification.xlsx', sheet_name='Patient-journey')

print("Categories of Emotions dataset:",emotions["Label"].unique())
print("Categories of Patient Journey dataset:",patient_journey["Label"].unique())

print(emotions.head()) 
print(patient_journey.head())

# Create list of splitted discussion texts
emotions_discussion_text = [re.split(r'[\.,\s\'\?\!:()\[\]]+',text) for text in emotions["discussion_text"]]
patient_journey_discussion_text = [re.split(r'[\.,\s\'\?\!:()\[\]]+',text) for text in patient_journey["discussion_text"]]

# Exclude stop words and short words
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

emotions_discussion_text = [[word.lower() for word in sent if (word.lower() not in stop_words and len(word) > 0)] for sent in emotions_discussion_text]
patient_journey_discussion_text = [[word.lower() for word in sent if (word.lower() not in stop_words and len(word) > 0)] for sent in patient_journey_discussion_text]

# Create (flat) list of all words in discussion texts
emotions_all_words = [word for wordlist in emotions_discussion_text for word in wordlist]
patient_journey_all_words = [word for wordlist in patient_journey_discussion_text for word in wordlist]

# the reviews will be stored as document pairs of words and category
emotions_docs = [(emotions_discussion_text[i], emotions["Label"][i]) for i in range(len(emotions))]

patient_journey_docs = [(patient_journey_discussion_text[i], patient_journey["Label"][i]) for i in range(len(patient_journey))]

#print(patient_journey_docs[0:2])

#give random order to the documents and print in table
random.seed(42)
random.shuffle(emotions_docs)
random.shuffle(patient_journey_docs)

tab = PrettyTable(['Document Features', 'Category'])
tab.horizontal_char = '-'

for (doc, cat) in emotions_docs[0:2]:
    feats = textwrap.fill(','.join(doc[:50]), width=40)
    tab.add_row([ feats, cat])
    tab.add_row([ '\n', '\n'])

print("Emotions")
print(tab)

tab = PrettyTable(['Document Features', 'Category'])
tab.horizontal_char = '-'

for (doc, cat) in patient_journey_docs[0:2]:
    feats = textwrap.fill(','.join(doc[:50]), width=40)
    tab.add_row([ feats, cat])
    tab.add_row([ '\n', '\n'])

print("Patient Journey")
print(tab)

# generate frequency distribution
# load all the words in freq distribution
emotions_freq_dist = nltk.FreqDist(w.lower() for w in emotions_all_words)
patient_journey_freq_dist = nltk.FreqDist(w.lower() for w in patient_journey_all_words)

#construct a list of the 2000 most frequent words in the overall corpus (you can try with other numbers as well)
emotions_most_freq_words = emotions_freq_dist.most_common(2000)
patient_journey_most_freq_words = patient_journey_freq_dist.most_common(2000)
#print('emotions most freq words: ', emotions_most_freq_words[100:110])

emotions_word_features = [word for (word, count) in emotions_most_freq_words]
patient_journey_word_features = [word for (word, count) in patient_journey_most_freq_words]
print('emotions word_features[:25]: ', emotions_word_features[:25])
print('patient journey word_features[:25]: ', patient_journey_word_features[:25])

# helper func to convert doc into a feature set
def get_document_features(document, doc_features):
    """
        This function will convert given document into a feature set.
        Note that we need to add the feature set that is relevant to the document we are inputting
        
    """
    #checking whether a word occurs in a set is much faster than checking whether it occurs in a list 
    document_words = set(document)
    features = {}
    
    #the feaures dict will consist of words as keys and boolean value of whether they exist in the document
    for word in doc_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features


# test code for the above function
emotions_words_doc = emotions_all_words

feat_dict = get_document_features(emotions_words_doc, emotions_word_features)

feat_dict_25 = {k: feat_dict[k] for k in list(feat_dict.keys())[:25]}
print('transformed document features for emotions data, printing the first 25 features \n\n', feat_dict_25)

# Preparing training set
#obtain feature sets for all discussion_texts
emotions_featuresets = [(get_document_features(d,emotions_word_features), c) for (d,c) in emotions_docs]
patient_journey_featuresets = [(get_document_features(d,patient_journey_word_features), c) for (d,c) in patient_journey_docs]

#split into train and test set (you can experiment with distribution here)
emotions_train_set, emotions_test_set = emotions_featuresets[100:], emotions_featuresets[:100]
patient_journey_train_set, patient_journey_test_set = patient_journey_featuresets[100:], patient_journey_featuresets[:100]

# Naive Bayes Classification
# instantiate classifier
emotions_nb_classifier = nltk.NaiveBayesClassifier.train(emotions_train_set)
patient_journey_nb_classifier = nltk.NaiveBayesClassifier.train(patient_journey_train_set)

"""### Logistic Regression"""

def get_key_categories(dictionary, value):
    """ 
    Get the labels names
    """
    if not value.isdigit():
        return None
    
    for key, val in dictionary.items():
        if val == int(value):
            return key 
    return None

def get_most_informative_features(clf, vectorizer, 
                                  label_names, 
                                  max_number_informative_features):
    """
    Prints features with the highest coefficient values, per class
    """
    output = []
    try:
        # get the feature name (the word)
        feature_names = vectorizer.get_feature_names_out()
        label_index = len(label_names)
        
        for index in range(label_index):
            output.append('\n' + label_names[index] + ':\n')
            # zip together feauteres with their respective importance coeficient
            coefs_with_feautures = sorted(zip(clf.coef_[index], feature_names))
            threshold = int(max_number_informative_features / 2)
            
            top = zip(coefs_with_feautures[:threshold],
                      coefs_with_feautures[:-(threshold + 1):-1])
                      
            for (coef_1, fn_1), (coef_2, fn_2) in top:
                feat = "\t%.4f\t%-15s\t\t%.4f\t%-15s" % (coef_1, fn_1, coef_2, fn_2)
                output.append(feat)
                
    except:
        print("Unexpected error:", sys.exc_info()[0])
        raise

    return '\n'.join(output)

journey_doc_lr = []
journey_labels_lr = []
journey_category_labels = []
journey_dictionary = {}
index = 0

# get the documents and labels in the respective lists, so they can be split into training and test sets
for text, label in patient_journey_docs:
    if label not in journey_dictionary:
            journey_dictionary[label] = index
            index += 1
    journey_doc_lr.append(" ".join(text))
    journey_labels_lr.append(journey_dictionary[label])

emotions_doc_lr = []
emotions_labels_lr = []
emotions_category_labels = []
emotions_dictionary = {}
index1 = 0

# get the documents and labels in the respective lists for the emotions data
for text, label in emotions_docs:
    if label not in emotions_dictionary:
        emotions_dictionary[label] = index1
        index1 += 1
    emotions_doc_lr.append(" ".join(text))
    emotions_labels_lr.append(emotions_dictionary[label])

# split datasets into training and testing 
X_train_journey, X_test_journey, y_train_journey, y_test_journey = journey_doc_lr[100:], journey_doc_lr[:100], journey_labels_lr[100:], journey_labels_lr[:100]
X_train_emotions, X_test_emotions, y_train_emotions, y_test_emotions = emotions_doc_lr[100:], emotions_doc_lr[:100], emotions_labels_lr[100:], emotions_labels_lr[:100]

# Patient journey logistic regression model pipeline
pipe_clf_journey = Pipeline(
    [('vect', CountVectorizer(max_features=100)),
     ('tfidf', TfidfTransformer()),
     ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial')),
     ])

# Emotions logistic regression model pipeline
pipe_clf_emotions = Pipeline(
    [('vect', CountVectorizer(max_features=100)),
     ('tfidf', TfidfTransformer()),
     ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial')),
     ])

# fit Logistic regression model with patient journey data
text_clf_journey = pipe_clf_journey.fit(X_train_journey,y_train_journey)

# fit Logistic regression model with emotions data
text_clf_journey = pipe_clf_emotions.fit(X_train_emotions,y_train_emotions)

# predict with patient journey data and get metrics information
predictions_journey = pipe_clf_journey.predict(X_test_journey)
measures_info_patient_journey = metrics.classification_report(y_test_journey, predictions_journey, output_dict=True)

# predict with emotions data and get metrics information
predictions_emotions = pipe_clf_journey.predict(X_test_emotions)
measures_info_emotions = metrics.classification_report(y_test_emotions, predictions_emotions, output_dict=True)

# get the pipeline elements of the patient journey logistic regression model to print the features afterwards
vectorizer_patient_journey = pipe_clf_journey.named_steps['vect']
clf_patient_journey = pipe_clf_journey.named_steps['clf']

# get the features and labels for patient journey data
feature_names_patient_journey = vectorizer_patient_journey.get_feature_names_out()
patient_journey_labels = list(journey_dictionary.keys())

# get the pipeline elements of the emotions logistic regression model
vectorizer_emotions = pipe_clf_emotions.named_steps['vect']
clf_emotions = pipe_clf_emotions.named_steps['clf']

# get the features and labels for emotions data
feature_names_emotions = vectorizer_emotions.get_feature_names_out()
emotions_labels = list(emotions_dictionary.keys())

"""### Comparison"""

# NB Accuracy
print("Naive Bayes Accuracy for emotions dataset:", nltk.classify.accuracy(emotions_nb_classifier, emotions_test_set))
print("Naive Bayes Accuracy for patient journey dataset:", nltk.classify.accuracy(patient_journey_nb_classifier, patient_journey_test_set)) 

# LR Accuracy
print('Logistic Regression Accuracy for emotions dataset :', round(accuracy_score(y_test_emotions, predictions_emotions),3))
print('Logistic Regression Accuracy for patient journey dataset :', round(accuracy_score(y_test_journey, predictions_journey),3))

"""- accuracy very similar for both classifiers
- small difference for emotions classification
recall for a better assessment of the performance)
- accuracy for patient journey classification in both cases higher than emotions classification as we have more data to train the models on (becomes even more important as we work with (unstructured) social media data)
- in general no very high accuracy for none of the classifications (would maybe need to have a look at precision and recall for a better assessment of the model performances)
"""

# NB most informative features
print("Naive Bayes most informative features for emotions dataset\n", emotions_nb_classifier.show_most_informative_features(20))
print("Naive Bayes most informative features for patient journey dataset\n",patient_journey_nb_classifier.show_most_informative_features(20))

# LR most informative features
important_features_emotions = get_most_informative_features(clf_emotions, vectorizer_emotions,emotions_labels, 10)
print('Naive Bayes most informative features for emotions data\n', important_features_emotions)
important_features_patient_journey = get_most_informative_features(clf_patient_journey, vectorizer_patient_journey,patient_journey_labels, 10)
print('Naive Bayes most informative features for patient journey data\n', important_features_patient_journey)

"""- features contain a lot of health-related terms and words that indicate that people describing experiences i.e. telling their story
- some features very expressive i.e. there is a clear relation observable between feature and label
- other features being more "random" - could be used in every context (could be improved by adding more preprocessing steps like e.g. excluding irrelevant features with a high informativeness value etc.) 
- LR features seem to make more sense as terms are even more expressive for each label (but we also included more features per label for LR in the above presentation)
"""